{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you define a few variables that could change as you attempt to optimize your model. \n",
    "\n",
    "### Often, these are just hard coded, or else provided as command line parameters once you know what variables you might be interested in varying.\n",
    "\n",
    "### Instead, we use a method to initialize these variables from either a config file or from command line parameters. This method is called by CANDLE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_url': 'ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/', 'train_data': 'nt_train2.csv', 'test_data': 'nt_test2.csv', 'model_name': 'nt3', 'conv': [128, 20, 1, 128, 10, 1], 'dense': [200, 20], 'activation': 'relu', 'out_act': 'softmax', 'loss': 'categorical_crossentropy', 'optimizer': 'sgd', 'metrics': 'accuracy', 'epochs': 1, 'batch_size': 20, 'learning_rate': 0.001, 'drop': 0.1, 'classes': 2, 'pool': [1, 10], 'save': '.', 'decay_rate': 0.0, 'config_file': '/Users/brettin/candle_tutorials/Topics/2_scripting/nt3_default_model.txt', 'datatype': <class 'numpy.float32'>}\n"
     ]
    }
   ],
   "source": [
    "import param_utils as p_utils\n",
    "def initialize_parameters():\n",
    "\n",
    "    # Get command-line parameters\n",
    "    parser = p_utils.get_nt3_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get parameters from configuration file\n",
    "    fileParameters = p_utils.read_config_file(args.config_file)\n",
    "\n",
    "    # Consolidate parameter set. Command-line parameters overwrite file configuration\n",
    "    gParameters = p_utils.args_overwrite_config(args, fileParameters)\n",
    "    return gParameters\n",
    "\n",
    "# HACK needed to parse command line params in notebook\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "gParameters = initialize_parameters()\n",
    "print(gParameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the data\n",
    "url_nt3 = gParameters['data_url']\n",
    "FILE_TRAIN = url_nt3 + gParameters['train_data']\n",
    "FILE_TEST = url_nt3  + gParameters['test_data']\n",
    "\n",
    "# Define the reference model\n",
    "CLASSES = gParameters['classes']\n",
    "DROPOUT_RATE = gParameters['drop']\n",
    "\n",
    "# Define optimizer\n",
    "OPTIMIZER=gParameters['optimizer']\n",
    "LEARNING_RATE = gParameters['learning_rate']\n",
    "DECAY_RATE = gParameters['decay_rate']\n",
    "\n",
    "# Compile the model\n",
    "METRICS=gParameters['metrics']\n",
    "LOSS='categorical_crossentropy'\n",
    "\n",
    "# Train the model (the optimized model has a default of 400 epochs)\n",
    "EPOCHS = gParameters['epochs']\n",
    "BATCH_SIZE = gParameters['batch_size']\n",
    "\n",
    "# Set up some variables for output files\n",
    "MODEL_NAME = gParameters['model_name']\n",
    "OUTPUT_DIR = gParameters['save']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you've set up your initial variables, it's time to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "\n",
    "    import threading\n",
    "    import queue\n",
    "    import sys\n",
    "    \n",
    "    def load_train(train_path, queue):\n",
    "        sys.stdout.write('looking for '+ train_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_train = (pd.read_csv(train_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading training data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_train)\n",
    "    \n",
    "    def load_test(test_path, queue):\n",
    "        sys.stdout.write('looking for ' + test_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_test = (pd.read_csv(test_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading test data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_test)\n",
    "\n",
    "    q1 = queue.Queue()\n",
    "    q2 = queue.Queue()\n",
    "    \n",
    "    thread1 = threading.Thread(name='load_train', target=load_train, args=(train_path, q1,))\n",
    "    thread2 = threading.Thread(name='load_test' , target=load_test, args=(test_path, q2,))\n",
    "    \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    df_train = q1.get()\n",
    "    df_test = q2.get()\n",
    "    \n",
    "    print('df_train shape:', df_train.shape)\n",
    "    print('df_test shape:', df_test.shape)\n",
    "\n",
    "    seqlen = df_train.shape[1]\n",
    "\n",
    "    df_y_train = df_train[:,0].astype('int')\n",
    "    df_y_test = df_test[:,0].astype('int')\n",
    "\n",
    "    # Convert a class vector (integers) to binary class matrix.\n",
    "    Y_train = np_utils.to_categorical(df_y_train,CLASSES)\n",
    "    Y_test = np_utils.to_categorical(df_y_test,CLASSES)\n",
    "\n",
    "    df_x_train = df_train[:, 1:seqlen].astype(np.float32)\n",
    "    df_x_test = df_test[:, 1:seqlen].astype(np.float32)\n",
    "\n",
    "    X_train = df_x_train\n",
    "    X_test = df_x_test\n",
    "\n",
    "    scaler = MaxAbsScaler()\n",
    "    mat = np.concatenate((X_train, X_test), axis=0)\n",
    "    mat = scaler.fit_transform(mat)\n",
    "\n",
    "    X_train = mat[:X_train.shape[0], :]\n",
    "    X_test = mat[X_train.shape[0]:, :]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This alows the code to executed through the run method as an imported package.\n",
    "\n",
    "### In the final version of nt3, the model is constructed dynamically from the config information in the nt3_default_model.txt file. You can see the final version at:\n",
    "\n",
    "https://github.com/ECP-CANDLE/Benchmarks/blob/frameworks/Pilot1/NT3/nt3_baseline_keras2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(gParameters):\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = load_data(FILE_TRAIN, FILE_TEST)\n",
    "    # this reshaping is critical for the Conv1D to work\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    num_params = X_train.shape[1]\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('Number of parameters: ', num_params)\n",
    "\n",
    "    # Define the reference model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=128, kernel_size=20, strides=1, padding='valid', input_shape=(num_params, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Conv1D(filters=128, kernel_size=10, strides=1, padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=10))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optimizers.SGD(lr=LEARNING_RATE, decay=DECAY_RATE)\n",
    "    \n",
    "    # Compile the model\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss=LOSS,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[METRICS])\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    csv_logger = CSVLogger('{}/training.log'.format(OUTPUT_DIR))\n",
    "\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "    print (datetime.datetime.now())\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks = [csv_logger, reduce_lr\n",
    "                                ])\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print (datetime.datetime.now())\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"{}/{}.model.json\".format(OUTPUT_DIR, MODEL_NAME), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "    print('Saved model to disk')\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"{}/{}.model.h5\".format(OUTPUT_DIR, MODEL_NAME))\n",
    "    print('Saved weights to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This allows the code to be executed at the command line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/nt_train2.csv\n",
      "looking for ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/nt_test2.csv\n",
      "done loading test data\n",
      "done loading training data\n",
      "df_train shape: (1120, 60484)\n",
      "df_test shape: (280, 60484)\n",
      "X_train shape: (1120, 60483, 1)\n",
      "X_test shape: (280, 60483, 1)\n",
      "Number of parameters:  60483\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 60464, 128)        2688      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60455, 128)        163968    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60455, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 6045, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 773760)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               154752200 \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 154,922,918\n",
      "Trainable params: 154,922,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2018-02-22 11:47:56.543815\n",
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "1120/1120 [==============================] - 1527s 1s/step - loss: 0.6904 - acc: 0.5473 - val_loss: 0.6867 - val_acc: 0.8036\n",
      "2018-02-22 12:14:50.336114\n",
      "Saved model to disk\n",
      "Saved weights to disk\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    gParameters = initialize_parameters()\n",
    "    run(gParameters)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    try:\n",
    "        K.clear_session()\n",
    "    except AttributeError:      # theano does not have this function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
