{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you define a few variables that could change as you attempt to optimize your model. \n",
    "\n",
    "### Often, these are just hard coded, or else provided as command line parameters once you know what variables you might be interested in varying.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the data\n",
    "url_nt3 = 'ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/'\n",
    "FILE_TRAIN = url_nt3 + 'nt_train2.csv'\n",
    "FILE_TEST = url_nt3  + 'nt_test2.csv'\n",
    "\n",
    "# Define the reference model\n",
    "CLASSES = 2\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# Define optimizer\n",
    "OPTIMIZER='sgd'\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_RATE = 0.\n",
    "\n",
    "# Compile the model\n",
    "METRICS='accuracy'\n",
    "LOSS='categorical_crossentropy'\n",
    "\n",
    "# Train the model (the optimized model has a default of 400 epochs)\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# Set up some variables for output files\n",
    "MODEL_NAME = 'nt3'\n",
    "OUTPUT_DIR = 'save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you've set up your initial variables, it's time to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "\n",
    "    import threading\n",
    "    import queue\n",
    "    \n",
    "    def load_train(train_path, queue):\n",
    "        sys.stdout.write('looking for '+ train_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_train = (pd.read_csv(train_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading training data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_train)\n",
    "    \n",
    "    def load_test(test_path, queue):\n",
    "        sys.stdout.write('looking for ' + test_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_test = (pd.read_csv(test_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading test data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_test)\n",
    "\n",
    "    q1 = queue.Queue()\n",
    "    q2 = queue.Queue()\n",
    "    \n",
    "    thread1 = threading.Thread(name='load_train', target=load_train, args=(train_path, q1,))\n",
    "    thread2 = threading.Thread(name='load_test' , target=load_test, args=(test_path, q2,))\n",
    "    \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    df_train = q1.get()\n",
    "    df_test = q2.get()\n",
    "    \n",
    "    print('df_train shape:', df_train.shape)\n",
    "    print('df_test shape:', df_test.shape)\n",
    "\n",
    "    seqlen = df_train.shape[1]\n",
    "\n",
    "    df_y_train = df_train[:,0].astype('int')\n",
    "    df_y_test = df_test[:,0].astype('int')\n",
    "\n",
    "    # Convert a class vector (integers) to binary class matrix.\n",
    "    Y_train = np_utils.to_categorical(df_y_train,CLASSES)\n",
    "    Y_test = np_utils.to_categorical(df_y_test,CLASSES)\n",
    "\n",
    "    df_x_train = df_train[:, 1:seqlen].astype(np.float32)\n",
    "    df_x_test = df_test[:, 1:seqlen].astype(np.float32)\n",
    "\n",
    "    X_train = df_x_train\n",
    "    X_test = df_x_test\n",
    "\n",
    "    scaler = MaxAbsScaler()\n",
    "    mat = np.concatenate((X_train, X_test), axis=0)\n",
    "    mat = scaler.fit_transform(mat)\n",
    "\n",
    "    X_train = mat[:X_train.shape[0], :]\n",
    "    X_test = mat[X_train.shape[0]:, :]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data(FILE_TRAIN, FILE_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this reshaping is critical for the Conv1D to work\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "num_params = X_train.shape[1]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Number of parameters: ', num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now define a convolutional neural network to classify tumor and normal tissue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the reference model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=20, strides=1, padding='valid', input_shape=(num_params, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Conv1D(filters=128, kernel_size=10, strides=1, padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optimizers.SGD(lr=LEARNING_RATE, decay=DECAY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=LOSS,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[METRICS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up to capture some output, in particular the model structure and weights so that these can be used to do inference.\n",
    "\n",
    "### Then go ahead and fit and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# path = '{}/{}.autosave.model.h5'.format(OUTPUT_DIR, MODEL_NAME)\n",
    "# checkpointer = ModelCheckpoint(filepath=path, verbose=1, save_weights_only=False, save_best_only=True)    csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n",
    "# candleRemoteMonitor = CandleRemoteMonitor(params=gParameters)\n",
    "\n",
    "csv_logger = CSVLogger('{}/training.log'.format(OUTPUT_DIR))\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks = [csv_logger, reduce_lr\n",
    "                                ])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, save the model structure and weights so you can use them in the future to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"{}/{}.model.json\".format(OUTPUT_DIR, MODEL_NAME), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "print('Saved model to disk')\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"{}/{}.model.h5\".format(OUTPUT_DIR, MODEL_NAME))\n",
    "print('Saved weights to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
