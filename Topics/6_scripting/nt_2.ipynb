{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/brettin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you define a few variables that could change as you attempt to optimize your model. \n",
    "\n",
    "### Often, these are just hard coded, or else provided as command line parameters once you know what variables you might be interested in varying.\n",
    "\n",
    "### Instead, we use a method to initialize these variables from either a config file or from command line parameters. This method is called by CANDLE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_url': 'ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/', 'train_data': 'nt_train2.csv', 'test_data': 'nt_test2.csv', 'model_name': 'nt3', 'conv': [128, 20, 1, 128, 10, 1], 'dense': [200, 20], 'activation': 'relu', 'out_act': 'softmax', 'loss': 'categorical_crossentropy', 'optimizer': 'sgd', 'metrics': 'accuracy', 'epochs': 1, 'batch_size': 20, 'learning_rate': 0.001, 'drop': 0.1, 'classes': 2, 'pool': [1, 10], 'save': '.', 'decay_rate': 0.0, 'config_file': '/Users/brettin/candle_tutorials/Topics/2_scripting/nt3_default_model.txt', 'datatype': <class 'numpy.float32'>}\n"
     ]
    }
   ],
   "source": [
    "import param_utils as p_utils\n",
    "def initialize_parameters():\n",
    "\n",
    "    # Get command-line parameters\n",
    "    parser = p_utils.get_nt3_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get parameters from configuration file\n",
    "    fileParameters = p_utils.read_config_file(args.config_file)\n",
    "\n",
    "    # Consolidate parameter set. Command-line parameters overwrite file configuration\n",
    "    gParameters = p_utils.args_overwrite_config(args, fileParameters)\n",
    "    return gParameters\n",
    "\n",
    "# HACK needed to parse command line params in notebook\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "gParameters = initialize_parameters()\n",
    "print(gParameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the data\n",
    "url_nt3 = gParameters['data_url']\n",
    "FILE_TRAIN = url_nt3 + gParameters['train_data']\n",
    "FILE_TEST = url_nt3  + gParameters['test_data']\n",
    "\n",
    "# Define the reference model\n",
    "CLASSES = gParameters['classes']\n",
    "DROPOUT_RATE = gParameters['drop']\n",
    "\n",
    "# Define optimizer\n",
    "OPTIMIZER=gParameters['optimizer']\n",
    "LEARNING_RATE = gParameters['learning_rate']\n",
    "DECAY_RATE = gParameters['decay_rate']\n",
    "\n",
    "# Compile the model\n",
    "METRICS=gParameters['metrics']\n",
    "LOSS='categorical_crossentropy'\n",
    "\n",
    "# Train the model (the optimized model has a default of 400 epochs)\n",
    "EPOCHS = gParameters['epochs']\n",
    "BATCH_SIZE = gParameters['batch_size']\n",
    "\n",
    "# Set up some variables for output files\n",
    "MODEL_NAME = gParameters['model_name']\n",
    "OUTPUT_DIR = gParameters['save']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you've set up your initial variables, it's time to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "\n",
    "    import threading\n",
    "    import queue\n",
    "    import sys\n",
    "    \n",
    "    def load_train(train_path, queue):\n",
    "        sys.stdout.write('looking for '+ train_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_train = (pd.read_csv(train_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading training data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_train)\n",
    "    \n",
    "    def load_test(test_path, queue):\n",
    "        sys.stdout.write('looking for ' + test_path + '\\n')\n",
    "        sys.stdout.flush()\n",
    "        df_test = (pd.read_csv(test_path,header=None).values).astype('float32')\n",
    "        sys.stdout.write('done loading test data\\n')\n",
    "        sys.stdout.flush()\n",
    "        queue.put(df_test)\n",
    "\n",
    "    q1 = queue.Queue()\n",
    "    q2 = queue.Queue()\n",
    "    \n",
    "    thread1 = threading.Thread(name='load_train', target=load_train, args=(train_path, q1,))\n",
    "    thread2 = threading.Thread(name='load_test' , target=load_test, args=(test_path, q2,))\n",
    "    \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    df_train = q1.get()\n",
    "    df_test = q2.get()\n",
    "    \n",
    "    print('df_train shape:', df_train.shape)\n",
    "    print('df_test shape:', df_test.shape)\n",
    "\n",
    "    seqlen = df_train.shape[1]\n",
    "\n",
    "    df_y_train = df_train[:,0].astype('int')\n",
    "    df_y_test = df_test[:,0].astype('int')\n",
    "\n",
    "    # Convert a class vector (integers) to binary class matrix.\n",
    "    Y_train = np_utils.to_categorical(df_y_train,CLASSES)\n",
    "    Y_test = np_utils.to_categorical(df_y_test,CLASSES)\n",
    "\n",
    "    df_x_train = df_train[:, 1:seqlen].astype(np.float32)\n",
    "    df_x_test = df_test[:, 1:seqlen].astype(np.float32)\n",
    "\n",
    "    X_train = df_x_train\n",
    "    X_test = df_x_test\n",
    "\n",
    "    scaler = MaxAbsScaler()\n",
    "    mat = np.concatenate((X_train, X_test), axis=0)\n",
    "    mat = scaler.fit_transform(mat)\n",
    "\n",
    "    X_train = mat[:X_train.shape[0], :]\n",
    "    X_test = mat[X_train.shape[0]:, :]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/nt_test2.csv\n",
      "looking for ftp://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/normal-tumor/nt_train2.csv\n",
      "done loading test data\n",
      "done loading training data\n",
      "df_train shape: (1120, 60484)\n",
      "df_test shape: (280, 60484)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data(FILE_TRAIN, FILE_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1120, 60483, 1)\n",
      "X_test shape: (280, 60483, 1)\n",
      "Number of parameters:  60483\n"
     ]
    }
   ],
   "source": [
    "# this reshaping is critical for the Conv1D to work\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "num_params = X_train.shape[1]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Number of parameters: ', num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now define a convolutional neural network to classify tumor and normal tissue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the reference model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=20, strides=1, padding='valid', input_shape=(num_params, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Conv1D(filters=128, kernel_size=10, strides=1, padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optimizers.SGD(lr=LEARNING_RATE, decay=DECAY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 60464, 128)        2688      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 60455, 128)        163968    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 60455, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 6045, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 773760)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               154752200 \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                4020      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 154,922,918\n",
      "Trainable params: 154,922,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=LOSS,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[METRICS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up to capture some output, in particular the model structure and weights so that these can be used to do inference.\n",
    "\n",
    "### Then go ahead and fit and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1bb778e697b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mreduce_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(X_train, Y_train,\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# path = '{}/{}.autosave.model.h5'.format(OUTPUT_DIR, MODEL_NAME)\n",
    "# checkpointer = ModelCheckpoint(filepath=path, verbose=1, save_weights_only=False, save_best_only=True)    csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n",
    "# candleRemoteMonitor = CandleRemoteMonitor(params=gParameters)\n",
    "\n",
    "csv_logger = CSVLogger('{}/training.log'.format(OUTPUT_DIR))\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks = [csv_logger, reduce_lr\n",
    "                                ])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, save the model structure and weights so you can use them in the future to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved weights to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"{}/{}.model.json\".format(OUTPUT_DIR, MODEL_NAME), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "print('Saved model to disk')\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"{}/{}.model.h5\".format(OUTPUT_DIR, MODEL_NAME))\n",
    "print('Saved weights to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
